{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Cm-wmMXY7vN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c96e0e9-83d1-4132-ed2b-9f30928dca86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef_hpj1K0lBs",
        "outputId": "a6aa2186-cc8e-426f-f18f-eb541177a8fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-uqyih79t\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-uqyih79t\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=5ad9dfe85545783b5febbe0b7ced2cc183359455d6a44e197bd2c89eea11835c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-54s2pww3/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8rX9ttmWHNf",
        "outputId": "5312e16d-b5d5-41c8-9cdc-fed7fb2aa024"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "//Global reduction kernel using more than one warp\n",
        "#include < iostream>    // cout, endl\n",
        "#include < numeric>     // accumulate\n",
        "#include < algorithm>   // iota, fill\n",
        "\n",
        "#define WARPSIZE (32)\n",
        "#define SDIV(x,y)(((x)+(y)-1)/(y))\n",
        "\n",
        "using value_t = unsigned int;\n",
        "\n",
        "constexpr size_t NUM_ELEMENTS = 1UL<<28;\n",
        "constexpr size_t BLOCK_SIZE = 512;\n",
        "\n",
        "template <typename T>\n",
        "__device__\n",
        "T my_warp_shfl_down(T var, unsigned int delta) {\n",
        "    return __shfl_down_sync(0xFFFFFFFF, var, delta, 32);\n",
        "}\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// FINISHED KERNEL (you dont have to change anything)\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__\n",
        "void global_reduction_kernel(const value_t * input, value_t * output, size_t inputSize)\n",
        "{\n",
        "    const size_t thid = blockDim.x*blockIdx.x + threadIdx.x;\n",
        "\n",
        "    value_t warpAccum = value_t(0); // here we store the warp result\n",
        "\n",
        "    // store entries in registers\n",
        "    if (thid < inputSize)\n",
        "        warpAccum = input[thid];\n",
        "\n",
        "    // reduce all values within a warp\n",
        "    for (size_t offset = WARPSIZE/2; offset>0; offset/=2)\n",
        "        warpAccum += my_warp_shfl_down(warpAccum, offset);\n",
        "\n",
        "    // first thread of every warp adds to global result\n",
        "    if (threadIdx.x % 32 == 0) atomicAdd(output, warpAccum);\n",
        "}\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// STUDENTS PART (fill in the gaps)\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__\n",
        "void shared_reduction_kernel(const value_t * input, value_t * output, size_t inputSize)\n",
        "{\n",
        "    __shared__ value_t blockAccum; // here we store the block result\n",
        "\n",
        "    const size_t thid = blockDim.x*blockIdx.x + threadIdx.x;\n",
        "\n",
        "    // TODO: initialize block result\n",
        "    \n",
        "\n",
        "    value_t warpAccum = value_t(0); // here we store the warp result\n",
        "\n",
        "    // store entries in registers\n",
        "    if (thid < inputSize)\n",
        "        warpAccum = input[thid];\n",
        "\n",
        "    // reduce all values within a warp\n",
        "    for (size_t offset = WARPSIZE/2; offset>0; offset/=2)\n",
        "        warpAccum += my_warp_shfl_down(warpAccum, offset);\n",
        "\n",
        "    // TODO: first thread of every warp adds to block result\n",
        "  \n",
        " \n",
        "    // TODO: first thread of every block adds to global result\n",
        " \n",
        "}\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// MAIN PROGRAM\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main () {\n",
        "\n",
        "    // choose GPU 0 (GTX 1080 (Pascal) 8GB RAM)\n",
        "    // or GPU 1 (Titan X (Maxwell) 12GB RAM)\n",
        "    cudaSetDevice(0);                                                     \n",
        "\n",
        "    // pointers to host arrays\n",
        "    value_t *h_array = nullptr;\n",
        "    value_t *h_sum = nullptr;\n",
        "    // pointers to device arrays\n",
        "    value_t *d_array = nullptr;\n",
        "    value_t *d_sum = nullptr;\n",
        "\n",
        "    const size_t arraySize = sizeof(value_t)*NUM_ELEMENTS;\n",
        "\n",
        "    // allocate memory\n",
        "    cudaMallocHost(&h_array, arraySize);                                      \n",
        "    cudaMallocHost(&h_sum, sizeof(value_t));                                \n",
        "    cudaMalloc(&d_array, arraySize);                                          \n",
        "    cudaMalloc(&d_sum, sizeof(value_t));                                     \n",
        "\n",
        "    std::fill(h_array, h_array+NUM_ELEMENTS, 1);\n",
        "\n",
        "    h_sum[0] = std::accumulate(h_array, h_array+NUM_ELEMENTS, 0);\n",
        " \n",
        "\n",
        "    // WARNING: this computes incorrect results!\n",
        "     h_sum[0] = 0;\n",
        "    for (size_t i = 0; i < NUM_ELEMENTS; i++)\n",
        "        h_sum[0] += h_array[i];\n",
        "    std::cout << '\\n';\n",
        "\n",
        "    cudaMemcpy(d_array, h_array, arraySize, cudaMemcpyHostToDevice);      \n",
        "    cudaMemset(d_sum, 0, sizeof(value_t));                                \n",
        " \n",
        "    // invoke the kernel\n",
        "    global_reduction_kernel<<< SDIV(NUM_ELEMENTS, BLOCK_SIZE), BLOCK_SIZE >>>\n",
        "        (d_array, d_sum, NUM_ELEMENTS);                                  \n",
        "  \n",
        "    // reset d_sum\n",
        "    cudaMemset(d_sum, 0, sizeof(value_t));                               \n",
        "\n",
        "    // invoke the kernel\n",
        "    shared_reduction_kernel<<< SDIV(NUM_ELEMENTS, BLOCK_SIZE), BLOCK_SIZE >>>\n",
        "        (d_array, d_sum, NUM_ELEMENTS);                                   \n",
        " \n",
        "     cudaMemcpy(h_sum, d_sum, sizeof(value_t), cudaMemcpyDeviceToHost);    \n",
        " \n",
        "    std::cout << '\\n';\n",
        "\n",
        "    // check if result computed correctly by CUDA\n",
        "    bool no_errors = true;\n",
        "    value_t truth = value_t(NUM_ELEMENTS);\n",
        "    if (h_sum[0] != truth) {\n",
        "        std::cout << \"error: got \" << h_sum[0] << \" but expected \" << truth << '\\n';\n",
        "        no_errors = false;\n",
        "    }\n",
        "\n",
        "    // free allocated memory\n",
        "    cudaFree(d_array);\n",
        "    cudaFree(d_sum);\n",
        "    cudaFreeHost(h_array);\n",
        "    cudaFreeHost(h_sum);\n",
        "\n",
        "    if(no_errors)\n",
        "        std::cout << \"CUDA programming is fun!\\n\";\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBfrjyzZWMiE",
        "outputId": "c8e68950-fc03-40e4-8b95-3e7ca6346afd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "error: got 0 but expected 268435456\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
