{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDb1eL1CW3XF",
        "outputId": "cff6d234-13c4-477a-86ed-ae3cfbaa731d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVjqZBj9XSvy",
        "outputId": "c13f5670-10b2-4bb5-f004-b79e96b5c692"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-r_8xvk8i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-r_8xvk8i\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=9fd661cb485cfd066c7b22a352e0e8a628034dc84b202ad47ff5b8d7ad8f6e66\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vki2o_ft/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs6xYdKBXVuc",
        "outputId": "1831d831-f9b8-48dc-9267-b0a5e94195d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "#include <cstdio>\n",
        "#include <vector>\n",
        "\n",
        "#define SDIV(x,y)(((x)+(y)-1)/(y))\n",
        "\n",
        "#define CUERR {                                                            \\\n",
        "        cudaError_t err;                                                       \\\n",
        "        if ((err = cudaGetLastError()) != cudaSuccess) {                       \\\n",
        "            std::cout << \"CUDA error: \" << cudaGetErrorString(err) << \" : \"    \\\n",
        "                    << __FILE__ << \", line \" << __LINE__ << std::endl;       \\\n",
        "            exit(1);                                                           \\\n",
        "        }                                                                      \\\n",
        "    }\n",
        "\n",
        "\n",
        "template <\n",
        "    typename index_t,\n",
        "    typename value_t>\n",
        "void load_binary(\n",
        "    const value_t * data,\n",
        "    const index_t length,\n",
        "    std::string filename) {\n",
        "\n",
        "    std::ifstream ifile(filename.c_str(), std::ios::binary);\n",
        "\n",
        "    if(!ifile.good()) {\n",
        "        throw std::runtime_error{\"can't open file \" + filename};\n",
        "    }\n",
        "\n",
        "    ifile.read((char*) data, sizeof(value_t)*length);\n",
        "    ifile.close();\n",
        "}\n",
        "\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// FINISHED KERNEL (you don't have to change anything)\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "template <\n",
        "    typename index_t,\n",
        "    typename value_t,\n",
        "    uint32_t chunk_size = 32>\n",
        "__global__\n",
        "void shared_covariance_kernel(\n",
        "    const value_t * data,\n",
        "    value_t * cov,\n",
        "    const index_t num_entries,\n",
        "    const index_t num_features)\n",
        "{\n",
        "    // convenience variables\n",
        "    const index_t base_x = blockIdx.x*chunk_size;\n",
        "    const index_t base_y = blockIdx.y*chunk_size;\n",
        "\n",
        "    const index_t thid_y = threadIdx.y;\n",
        "    const index_t thid_x = threadIdx.x;\n",
        "\n",
        "    const index_t x = base_x + thid_x;\n",
        "    const index_t y = base_y + thid_y;\n",
        "\n",
        "    // optional early exit: -500ms\n",
        "    if (base_x > base_y) return;\n",
        "\n",
        "    // allocate shared memory\n",
        "    __shared__ value_t s_cache_x[chunk_size][chunk_size];\n",
        "    __shared__ value_t s_cache_y[chunk_size][chunk_size];\n",
        "\n",
        "    // compute the number of chunks to be computed\n",
        "    const index_t num_chunks = SDIV(num_entries, chunk_size);\n",
        "\n",
        "    // accumulated value of scalar product\n",
        "    value_t accum = 0;\n",
        "\n",
        "    // for each chunk\n",
        "    for (index_t chunk = 0; chunk < num_chunks; chunk++) {\n",
        "\n",
        "            // assign thread IDs to rows and columns\n",
        "            const index_t row   = thid_y + chunk*chunk_size;\n",
        "            const index_t col_x = thid_x + base_x;\n",
        "            const index_t col_y = thid_x + base_y;\n",
        "\n",
        "            // check if valid row or column indices\n",
        "            const bool valid_row   = row   < num_entries;\n",
        "            const bool valid_col_x = col_x < num_features;\n",
        "            const bool valid_col_y = col_y < num_features;\n",
        "\n",
        "            // fill shared memory with tiles where thid_y enumerates\n",
        "            // image identifiers (entries) and thid_x denotes feature\n",
        "            // coordinates (pixels). s_cache_x corresponds to x and\n",
        "            // s_cache_y to y where cov[x,y] is the pairwise covariance\n",
        "            s_cache_x[thid_y][thid_x] = valid_row*valid_col_x ?\n",
        "                                      data[row*num_features+col_x] : 0;\n",
        "            s_cache_y[thid_y][thid_x] = valid_row*valid_col_y ?\n",
        "                                      data[row*num_features+col_y] : 0;\n",
        "\n",
        "            // this is needed to ensure that all threads finished writing\n",
        "            // shared memory\n",
        "            __syncthreads();\n",
        "\n",
        "            // optional early exit: -100ms\n",
        "            if (x <= y)\n",
        "                // here we actually evaluate the scalar product\n",
        "                for (index_t entry = 0; entry < chunk_size; entry++)\n",
        "                    accum += s_cache_y[entry][thid_y]*s_cache_x[entry][thid_x];\n",
        "\n",
        "            // this is needed to ensure that shared memory can be over-\n",
        "            // written again in the next iteration\n",
        "            __syncthreads();\n",
        "    }\n",
        "\n",
        "    // since cov[x,y] = cov[y,x] we only compute one entry\n",
        "    if (y < num_features && x <= y)\n",
        "        cov[y*num_features+x] =\n",
        "        cov[x*num_features+y] = accum;//num_entries;\n",
        "\n",
        "}\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// DATA STRUCTURES\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "template <uint64_t num_gpus, uint64_t num_streams>\n",
        "struct partition {\n",
        "    static constexpr uint64_t num_slots = num_gpus*num_streams;\n",
        "    uint64_t offsets[num_slots];\n",
        "    uint64_t counts[num_slots];\n",
        "\n",
        "    partition(uint64_t length) {\n",
        "\n",
        "        uint64_t batch_size = (length+num_slots-1)/num_slots;\n",
        "\n",
        "        for (uint64_t gpu = 0; gpu < num_gpus; gpu++) {\n",
        "            for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
        "                const uint64_t slot = gpu*num_streams+stream;\n",
        "                const uint64_t lower = slot*batch_size;\n",
        "                const uint64_t upper = std::min(lower+batch_size, length);\n",
        "                const uint64_t count = upper-lower;\n",
        "\n",
        "                offsets[slot] = lower;\n",
        "                counts[slot] = count;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    uint64_t get_size(uint64_t gpu, uint64_t stream) const {\n",
        "        return counts[gpu*num_streams+stream];\n",
        "    }\n",
        "\n",
        "    uint64_t get_offset(uint64_t gpu, uint64_t stream) const {\n",
        "        return offsets[gpu*num_streams+stream];\n",
        "    }\n",
        "};\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// MAIN PROGRAM (take a look at what the program does)\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main () {\n",
        "\n",
        "    constexpr uint64_t num_images = 10000, num_rows = 55, num_cols = 45;\n",
        "    constexpr uint64_t num_pixels = num_rows * num_cols;\n",
        "\n",
        "    // pointer for data matrix and mean vector\n",
        "    float * h_data = nullptr;\n",
        "    cudaMallocHost(&h_data, sizeof(float)*num_images*num_pixels); \n",
        "    load_binary(h_data, num_images*num_pixels, \"/content/celebA_centered.10000.bin\");\n",
        "                \n",
        "    std::cout << \"Load input file\" << std::endl;\n",
        "    constexpr uint64_t num_gpus = 2, num_streams = 4;\n",
        "    partition< num_gpus, num_streams > part(num_images);\n",
        "\n",
        "    float * d_data[num_gpus][num_streams];\n",
        "    float * d_cov [num_gpus][num_streams];\n",
        "    float * h_cov [num_gpus][num_streams];\n",
        "\n",
        "    cudaStream_t streams[num_gpus][num_streams];\n",
        "\n",
        "    for (uint64_t gpu = 0; gpu < num_gpus; gpu++) {\n",
        "        cudaSetDevice(0);\n",
        "        for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
        "\n",
        "            const uint64_t part_size = part.get_size(gpu, stream);\n",
        "            const uint64_t part_bytes = sizeof(float)*part_size*num_pixels;\n",
        "            const uint64_t cov_bytes = sizeof(float)*num_pixels*num_pixels;\n",
        "\n",
        "            cudaStreamCreate(&streams[gpu][stream]);\n",
        "            cudaMalloc    (&d_data[gpu][stream], part_bytes); CUERR\n",
        "            cudaMalloc    (&d_cov[gpu][stream], cov_bytes); CUERR\n",
        "            cudaMallocHost(&h_cov[gpu][stream], cov_bytes); CUERR\n",
        "            cudaMemset    (d_data[gpu][stream], 0, part_bytes); CUERR\n",
        "            cudaMemset    (d_cov[gpu][stream], 0, cov_bytes); CUERR\n",
        "            cudaMemset    (h_cov[gpu][stream], 0, cov_bytes); CUERR\n",
        "        }\n",
        "    } \n",
        "   std::cout << \"Memory and streams init\" << std::endl;\n",
        " \n",
        "    ///////////////////////////////////////////////////////////////////////////\n",
        "    // STUDENTS PART (fill in the gaps)\n",
        "    ///////////////////////////////////////////////////////////////////////////\n",
        "    for (uint64_t gpu = 0; gpu < num_gpus; gpu++) {\n",
        "        cudaSetDevice(0);\n",
        "        for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
        "            // offset where the part begins in h_data\n",
        "            const uint64_t part_offset = part.get_offset(gpu, stream) * num_pixels;\n",
        "            // number of images in the part\n",
        "            const uint64_t part_size   = part.get_size(gpu, stream);\n",
        "\n",
        "            const uint64_t part_bytes  = sizeof(float)*part_size*num_pixels;\n",
        "\n",
        "            ///////////////////////////////////////////////////////////////////\n",
        "            // copy data from h_data to d_data using a different stream for each part\n",
        "            //cudaMemcpyAsync(...);\n",
        "            cudaMemcpyAsync(d_data[gpu][stream], h_data+part_offset, part_bytes,\n",
        "                            cudaMemcpyHostToDevice, streams[gpu][stream]);\n",
        "            ///////////////////////////////////////////////////////////////////\n",
        "  \n",
        "            const dim3 blocks(SDIV(num_pixels, 32), SDIV(num_pixels, 32));\n",
        "            const dim3 threads(32, 32);\n",
        "            ///////////////////////////////////////////////////////////////////\n",
        "            // call shared_covariance_kernel in the stream\n",
        "            //shared_covariance_kernel<<<...>>>\n",
        "            //   (d_data[gpu][stream], d_cov[gpu][stream], part_size, num_pixels);\n",
        "            shared_covariance_kernel<<< blocks, threads, 0, streams[gpu][stream] >>>\n",
        "                (d_data[gpu][stream], d_cov[gpu][stream], part_size, num_pixels);\n",
        "            ///////////////////////////////////////////////////////////////////\n",
        "   \n",
        "            const uint64_t cov_bytes = sizeof(float)*num_pixels*num_pixels;\n",
        "            ///////////////////////////////////////////////////////////////////\n",
        "            // copy results from d_cov to h_cov gpu using the stream\n",
        "            //cudaMemcpyAsync(...);\n",
        "            cudaMemcpyAsync(h_cov[gpu][stream], d_cov[gpu][stream], cov_bytes,\n",
        "                            cudaMemcpyDeviceToHost, streams[gpu][stream]);\n",
        "            ///////////////////////////////////////////////////////////////////\n",
        "         }\n",
        "    } \n",
        "    ///////////////////////////////////////////////////////////////////////////\n",
        "    std::cout << \"Computation\" << std::endl;\n",
        " \n",
        "\n",
        "    for (uint64_t gpu = 0; gpu < num_gpus; gpu++) {\n",
        "        cudaSetDevice(0);\n",
        "        for (uint64_t stream = 0; stream < num_streams; stream++)\n",
        "            cudaStreamSynchronize(streams[gpu][stream]);                  \n",
        "    }\n",
        "   std::cout << \"Stream sync\" << std::endl;\n",
        " \n",
        "    std::vector<float> h_result(num_pixels*num_pixels);  \n",
        "    std::vector<float> h_truth(num_pixels*num_pixels);  \n",
        "    //float * h_result = nullptr, \n",
        "    //float * h_truth = nullptr;\n",
        "    //cudaMallocHost(&h_result, sizeof(float)*num_pixels*num_pixels);       \n",
        "    //cudaMallocHost(&h_truth,  sizeof(float)*num_pixels*num_pixels);      \n",
        "    //cudaMemset(h_result, 0, sizeof(float)*num_pixels*num_pixels);         \n",
        " \n",
        "    for (uint64_t i = 0; i < num_pixels*num_pixels; i++)\n",
        "        for (uint64_t gpu = 0; gpu < num_gpus; gpu++)\n",
        "            for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
        "                h_result[i] += h_cov[gpu][stream][i];\n",
        "            }\n",
        "             \n",
        "   std::cout << \"Add partials\" << std::endl;\n",
        " \n",
        "    for (uint64_t i = 0; i < num_pixels*num_pixels; i++)\n",
        "        h_result[i] /= num_images;\n",
        "\n",
        "   std::cout << \"Start error check\" << std::endl;\n",
        "\n",
        "    load_binary(h_truth.data(), num_pixels*num_pixels, \"/content/celebA_covariance.10000.bin\");\n",
        "    std::cout << \"Loaded truth file\" << std::endl;\n",
        "    bool no_errors = true;\n",
        "    for (uint64_t i = 0; i < num_pixels*num_pixels; i++) {\n",
        "        const auto res  = h_result[i] -  h_truth[i];\n",
        "        if (res*res > 10) {\n",
        "            std::cout <<  \"ERROR: \" <<  h_result[i] << \" \" << h_truth[i]\n",
        "                      << \" \" << (i % (num_pixels))\n",
        "                      << \" \" << (i / (num_pixels)) << std::endl;\n",
        "            no_errors = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "   std::cout << \"End error check\" << std::endl;\n",
        " \n",
        "    for(uint64_t gpu = 0; gpu < num_gpus; gpu++) {\n",
        "        cudaSetDevice(0);\n",
        "        for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
        "            cudaFree    (d_data[gpu][stream]);\n",
        "            cudaFree    (d_cov[gpu][stream]);\n",
        "            cudaFreeHost(h_cov[gpu][stream]);\n",
        "            cudaStreamSynchronize(streams[gpu][stream]);\n",
        "            cudaStreamDestroy(streams[gpu][stream]);\n",
        "        }\n",
        "    } \n",
        "\n",
        "    cudaFreeHost(h_data);                                                 \n",
        "    std::cout << '\\n';\n",
        "\n",
        "    if(no_errors)\n",
        "        std::cout << \"CUDA is fun\" << std::endl;\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4m1O6xeXeFd",
        "outputId": "c53ae479-fd48-4d28-aec4-013564dddaee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terminate called after throwing an instance of 'std::runtime_error'\n",
            "  what():  can't open file /content/celebA_centered.10000.bin\n",
            "\n"
          ]
        }
      ]
    }
  ]
}