{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kyeef8HtsX8",
        "outputId": "63328e61-6edb-4ac6-9b93-b3cb6a1a8180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-frv_vf89\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-frv_vf89\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=8f86614ec389184b55ed29c1a1e4426e0397dd6b3b4838c2398e2996577fe82d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wn2xajs4/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPXOv1LKyGZd",
        "outputId": "ddbbb793-0cdc-4292-d5c9-5f4bee0b3d6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHFOwgRAvWkI",
        "outputId": "bb46ad69-9cb3-4a68-eeaa-9f9731752a3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "    int\n",
        "    main()\n",
        "{\n",
        "    std::cout << \"Welcome To GeeksforGeeks\\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCwu8tv7vcaA",
        "outputId": "31c1552c-c3d6-47e4-b50c-e2684c4277bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome To GeeksforGeeks\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        " \n",
        "    using namespace std;\n",
        " \n",
        "__global__ void maxi(int* a, int* b, int n)\n",
        "{\n",
        "    int block = 256 * blockIdx.x;\n",
        "    int max = 0;\n",
        " \n",
        "    for (int i = block; i < min(256 + block, n); i++) {\n",
        " \n",
        "        if (max < a[i]) {\n",
        "            max = a[i];\n",
        "        }\n",
        "    }\n",
        "    b[blockIdx.x] = max;\n",
        "}\n",
        " \n",
        "int main()\n",
        "{\n",
        " \n",
        "    int n;\n",
        "    n = 100;\n",
        "    int a[n];\n",
        " \n",
        "    for (int i = 0; i < n; i++) {\n",
        "        a[i] = rand() % n;\n",
        "        cout << a[i] << \"\\t\";\n",
        "    }\n",
        " \n",
        "    cudaEvent_t start, end;\n",
        "    int *ad, *bd;\n",
        "    int size = n * sizeof(int);\n",
        "    cudaMalloc(&ad, size);\n",
        "    cudaMemcpy(ad, a, size, cudaMemcpyHostToDevice);\n",
        "    int grids = ceil(n * 1.0f / 256.0f);\n",
        "    cudaMalloc(&bd, grids * sizeof(int));\n",
        " \n",
        "    dim3 grid(grids, 1);\n",
        "    dim3 block(1, 1);\n",
        " \n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&end);\n",
        "    cudaEventRecord(start);\n",
        " \n",
        "    while (n > 1) {\n",
        "        maxi<<<grids, block>>>(ad, bd, n);\n",
        "        n = ceil(n * 1.0f / 256.0f);\n",
        "        cudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "    }\n",
        " \n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        " \n",
        "    float time = 0;\n",
        "    cudaEventElapsedTime(&time, start, end);\n",
        " \n",
        "    int ans[2];\n",
        "    cudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost);\n",
        " \n",
        "    cout << \"The maximum element is : \" << ans[0] << endl;\n",
        " \n",
        "    cout << \"The time required : \";\n",
        "    cout << time << endl;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USxjKhrAviWj",
        "outputId": "abb181a1-6415-4226-ad4c-b30a2d1d85b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\t86\t77\t15\t93\t35\t86\t92\t49\t21\t62\t27\t90\t59\t63\t26\t40\t26\t72\t36\t11\t68\t67\t29\t82\t30\t62\t23\t67\t35\t29\t2\t22\t58\t69\t67\t93\t56\t11\t42\t29\t73\t21\t19\t84\t37\t98\t24\t15\t70\t13\t26\t91\t80\t56\t73\t62\t70\t96\t81\t5\t25\t84\t27\t36\t5\t46\t29\t13\t57\t24\t95\t82\t45\t14\t67\t34\t64\t43\t50\t87\t8\t76\t78\t88\t84\t3\t51\t54\t99\t32\t60\t76\t68\t39\t12\t26\t86\t94\t39\tThe maximum element is : 99\n",
            "The time required : 0.032512\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>  // cout, endl\n",
        "#include <numeric>   // iota, fill\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// STUDENTS PART (feel free to code)\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "// 2^27 float elements need 1.5 gigabytes of memory (2*0.5GB input, 0.5GB output)\n",
        "// You can set numElements to 1024 if you only want to test a single block\n",
        "// constexpr size_t numElements = 1024;\n",
        "constexpr size_t numElements = 1UL<<27;\n",
        "\n",
        "// write a kernel where each thread calculates the sum of two input values and\n",
        "// stores the result in the output array\n",
        "__global__\n",
        "void add_kernel(const float * a_in, const float * b_in, float * c_out, size_t n)\n",
        "{\n",
        "    // your code\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if(tid < n)\n",
        "        c_out[tid] = a_in[tid] + b_in[tid];\n",
        "}\n",
        "\n",
        "// if you are bored try to write the kernel where each thread calculates multiple\n",
        "// additions using a for loop\n",
        "// (you have to uncomment the kernel in the main function to use it)\n",
        "__global__\n",
        "void strided_add_kernel(const float * a_in, const float * b_in, float * c_out, size_t n)\n",
        "{\n",
        "    // your code\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    for(int i = tid; i < n; i += blockDim.x * gridDim.x)\n",
        "        c_out[i] = a_in[i] + b_in[i];\n",
        "}\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "// MAIN PROGRAM (take a look at what the program does)\n",
        "///////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main () {\n",
        "    // choose GPU 0 (GTX 1080 (Pascal) 8GB RAM)\n",
        "    // or GPU 1 (Titan X (Maxwell) 12GB RAM)\n",
        "    cudaSetDevice(0);                                                     \n",
        "\n",
        "    // pointers to host arrays\n",
        "    float * h_a = nullptr;\n",
        "    float * h_b = nullptr;\n",
        "    float * h_c = nullptr;\n",
        "    // pointers to device arrays\n",
        "    float * d_a = nullptr;\n",
        "    float * d_b = nullptr;\n",
        "    float * d_c = nullptr;\n",
        "\n",
        "    const size_t arraySize = sizeof(float)*numElements;\n",
        "\n",
        "    // allocate pinned host memory\n",
        "    cudaMallocHost(&h_a, arraySize);                                     \n",
        "    cudaMallocHost(&h_b, arraySize);                                     \n",
        "    cudaMallocHost(&h_c, arraySize);                                    \n",
        "\n",
        "    // allocate device memory\n",
        "    cudaMalloc(&d_a, arraySize);                                        \n",
        "    cudaMalloc(&d_b, arraySize);                                        \n",
        "    cudaMalloc(&d_c, arraySize);                                        \n",
        "\n",
        "    // fill h_a and h_b with stuff\n",
        "    std::iota(h_a, h_a+numElements, 0);             // (0, 1, 2, 3, ..., N-1)\n",
        "    std::fill(h_b, h_b+numElements, 1);             // (1, 1, 1, 1, ..., 1)\n",
        " \n",
        "    // measure time for vector addition on single-threaded host\n",
        "     for (size_t index = 0; index < numElements; index++)\n",
        "        h_c[index] = h_a[index] + h_b[index];\n",
        "  \n",
        "    // measure time for vector addition on multi-threaded host\n",
        "    for (size_t index = 0; index < numElements; index++)\n",
        "        h_c[index] = h_a[index] + h_b[index];\n",
        "    std::cout << '\\n';\n",
        "\n",
        "     // copy data from host to device\n",
        "    cudaMemcpy(d_a, h_a, arraySize, cudaMemcpyHostToDevice);             \n",
        "    cudaMemcpy(d_b, h_b, arraySize, cudaMemcpyHostToDevice);             \n",
        "  \n",
        "    // Note, the next line is not needed in practice. However, we overwrite\n",
        "    // the device vector d_c to prevent spurious false positives. As an example,\n",
        "    // if another student writes the correct result to d_c and the GPU assigns\n",
        "    // the same address range during your run (this happens quite often) then\n",
        "    // you might pass the test below even if you process nothing!\n",
        "    cudaMemset(d_c, 0, arraySize);                                      \n",
        "\n",
        "    // invoke the kernel\n",
        "    int threadsPerBlock = 1024;\n",
        "    int numBlocks = (numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    add_kernel<<<numBlocks, threadsPerBlock>>>(d_a, d_b, d_c, numElements);\n",
        " \n",
        "    // uncomment the following lines for the strided kernel\n",
        "    threadsPerBlock = 1024;\n",
        "    numBlocks = 1024;\n",
        "    strided_add_kernel<<<numBlocks, threadsPerBlock>>>(d_a, d_b, d_c, numElements);\n",
        " \n",
        "    // copy result from device to host\n",
        "     cudaMemcpy(h_c, d_c, arraySize, cudaMemcpyDeviceToHost);            \n",
        "\n",
        "     // check if result computed correctly by CUDA\n",
        "    bool no_errors = true;\n",
        "    for (size_t index = 0; index < numElements; index++) {\n",
        "        if (h_c[index] != h_a[index] + h_b[index]) {\n",
        "            std::cout << \"first error at position \" << index << std::endl;\n",
        "            no_errors = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // free memory allocations\n",
        "    cudaFreeHost(h_a);\n",
        "    cudaFreeHost(h_b);\n",
        "    cudaFreeHost(h_c);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        " \n",
        "    if(no_errors)\n",
        "        std::cout << \"CUDA programming is fun!\" << std::endl;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbfIBn3py2QF",
        "outputId": "d13dec7b-23f4-4fe9-a23a-2844aac68c32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA programming is fun!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}